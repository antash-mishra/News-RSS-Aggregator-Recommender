# -*- coding: utf-8 -*-
"""TFIDF Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qM7eHB9fAfeJtoldAOakHiANncHaOIWd
"""

import pandas as pd

data = pd.read_csv('news.tsv', header=None, sep='\t')

data.columns=['News ID',
"Category",
"SubCategory",
"Title",
"Abstract",
"URL",
"Title Entities",
"Abstract Entities "]

data.head()

data=data.iloc[:,:5]

data

import numpy as np

c=data[['Category','SubCategory']].value_counts()

index=[]
for i in c.index:
    index.append(np.array(i))
index=np.array(index)

print('The number of articles: ', len(data))
data.drop_duplicates(subset = ['Title'], inplace = True)
print("No. of articles after removing duplicates: ", len(data))

data.isna().sum()

data.dropna(inplace=True)

data.isna().sum()

print('the number of articles before processing :',len(data))
data=data[data['Title'].apply((lambda x: len(x.split())>=4))]
print('The number of articles after processing :',len(data))

df2 = data.copy()

"""### Data Preprocessing"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
import re
import spacy

# Download NLTK resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Lowercasing
    tokens = [token.lower() for token in tokens]

    # Remove punctuation
    tokens = [token for token in tokens if token not in string.punctuation]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatization using NLTK
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Remove non-alphabetic characters
    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]

    # Remove empty tokens
    tokens = [token for token in tokens if token.strip() != '']

    # Join tokens back into text
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

data['Title'] = data['Title'].apply(preprocess_text)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_headline_vectorizer = TfidfVectorizer(min_df = 0)

from sklearn.metrics import pairwise_distances

def TFIDF_based_model(row_index, num_similar_items):
    cate=data['Category'][row_index]
    name=data['Title'][row_index]
    cate_data=data[data['Category']==cate]

    row_index2=cate_data[cate_data['Title']==name].index
    headline_features   = tfidf_headline_vectorizer.fit_transform(cate_data['Title'].values)
    couple_dist = pairwise_distances(headline_features,headline_features[row_index2])
    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]
    df = pd.DataFrame({'headline':df2[df2['Category']==cate]['Title'].values[indices],
                       'Category':cate_data['Category'].values[indices],
                       'Abstract':cate_data['Abstract'].values[indices],
                'Euclidean Distance Similarity': couple_dist[indices].ravel()})
    print("="*30,"News Article Name","="*30)
    print('News Headline : ',data['Title'][indices[0]])
    print("\n","="*26,"Recommended News Using TFIDf: ","="*30)
    return df.iloc[1:,:]